{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn\n",
    "seaborn.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dacbench.benchmarks import ModCMABenchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First steps: running an episode\n",
    "\n",
    "### Creating a benchmark object\n",
    "Benchmarks are environments created by a benchmark object.\n",
    "First, we take a look at that object and the configuration it holds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config_space: Configuration space object:\n",
      "  Hyperparameters:\n",
      "    0_active, Type: Categorical, Choices: {True, False}, Default: True\n",
      "    1_elitist, Type: Categorical, Choices: {True, False}, Default: True\n",
      "    2_orthogonal, Type: Categorical, Choices: {True, False}, Default: True\n",
      "    3_sequential, Type: Categorical, Choices: {True, False}, Default: True\n",
      "    4_threshold_convergence, Type: Categorical, Choices: {True, False}, Default: True\n",
      "    5_step_size_adaption, Type: Categorical, Choices: {csa, tpa, msr, xnes, m-xnes, lp-xnes, psr}, Default: csa\n",
      "    6_mirrored, Type: Categorical, Choices: {None, mirrored, mirrored pairwise}, Default: None\n",
      "    7_base_sampler, Type: Categorical, Choices: {gaussian, sobol, halton}, Default: gaussian\n",
      "    8_weights_option, Type: Categorical, Choices: {default, equal, 1/2^lambda}, Default: default\n",
      "    90_local_restart, Type: Categorical, Choices: {None, IPOP, BIPOP}, Default: None\n",
      "    91_bound_correction, Type: Categorical, Choices: {None, saturate, unif_resample, COTN, toroidal, mirror}, Default: None\n",
      "\n",
      "action_space_class: MultiDiscrete\n",
      "action_space_args: [[2, 2, 2, 2, 2, 7, 3, 3, 3, 3, 6]]\n",
      "observation_space_class: Box\n",
      "observation_space_args: [array([-inf, -inf, -inf, -inf, -inf]), array([inf, inf, inf, inf, inf])]\n",
      "observation_space_type: <class 'numpy.float32'>\n",
      "reward_range: (-1000000000000, 0)\n",
      "budget: 100\n",
      "cutoff: 1000000.0\n",
      "seed: 0\n",
      "step_size: False\n",
      "multi_agent: False\n",
      "instance_set_path: /home/missi/Documents/work/DACBench/dacbench/benchmarks/../instance_sets/modea/modea_train.csv\n",
      "test_set_path: /home/missi/Documents/work/DACBench/dacbench/benchmarks/../instance_sets/modea/modea_train.csv\n",
      "benchmark_info: {'identifier': 'ModCMA', 'name': 'Online Selection of CMA-ES Variants', 'reward': 'Negative best function value', 'state_description': ['Generation Size', 'Sigma', 'Remaining Budget', 'Function ID', 'Instance ID']}\n"
     ]
    }
   ],
   "source": [
    "benchCMA = ModCMABenchmark()\n",
    "for k in benchCMA.config.keys():\n",
    "    print(f\"{k}: {benchCMA.config[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the benchmark environment\n",
    "Now we can either get the default benchmark setting like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = benchCMA.get_benchmark(seed=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the benchmark\n",
    "To execute a run, first reset the environment. It will return an initial state as well as a dictonary that may or may not contain some meta-information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.   2. 100.  11.   0.]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "state, info = env.reset()\n",
    "print(state)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can run steps until the algorithm run is done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1...........................................Reward: -141433.68262540782\n"
     ]
    }
   ],
   "source": [
    "terminated, truncated = False, False\n",
    "cum_reward = 0\n",
    "while not (terminated or truncated):\n",
    "    action = env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = env.step(action)\n",
    "    cum_reward += reward\n",
    "print(f\"Episode 1/1...........................................Reward: {cum_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test run the environment for step size\n",
    "Change the config so that step_size = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchStepCMA = ModCMABenchmark(step_size=True)\n",
    "for k in benchStepCMA.config.keys():\n",
    "    print(f\"{k}: {benchStepCMA.config[k]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_env = benchStepCMA.get_benchmark(seed=1)\n",
    "benchStepCMA.config.step_size = True\n",
    "step_env = benchStepCMA.get_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 10.   2. 100.  11.   0.]\n",
      "{}\n"
     ]
    }
   ],
   "source": [
    "state, info = step_env.reset()\n",
    "print(state)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1...........................................Reward: -518064.4693024508\n"
     ]
    }
   ],
   "source": [
    "terminated, truncated = False, False\n",
    "cum_reward = 0\n",
    "while not (terminated or truncated):\n",
    "    action = step_env.action_space.sample()\n",
    "    state, reward, terminated, truncated, info = step_env.step(action)\n",
    "    cum_reward += reward\n",
    "print(f\"Episode 1/1...........................................Reward: {cum_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacbench_migration",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
